# Value Proposition
## Why Choose Our Team for the Graphwise Benchmarking Study

**Project**: GW-001

**Prepared By**: University of Washington MSIM Team - **The Big 4**

**Date**: November 1, 2025

**Version**: 1.0

---

## Executive Summary

**The Opportunity**: Graphwise operates in a **$48.4B market by 2030**, yet faces a critical positioning challenge—no standardized benchmarks validate the company's GraphRAG superiority vs. competitors like Neo4j, AWS Neptune, and Stardog.

**Our Solution**: A **20-week academic-quality benchmarking study** delivering three simultaneous value streams:

1. **Academic Credibility**: Peer-review-ready research paper establishing Graphwise as thought leader
2. **Competitive Intelligence**: Quantified performance data across 6 platforms informing FY2026 strategy
3. **Sales Enablement**: Independent validation replacing vendor marketing claims with empirical evidence

**Our Team's Unique Value**: We combine **87% project match** (academic excellence + enterprise database expertise + vendor-neutral objectivity) with **proven collaboration** (co-founded ₹3M profit startup) to deliver **$125K+ consulting value** as an academic capstone project.

**Bottom Line**: You get McKinsey/BCG-quality research at academic pricing, positioning Graphwise for market leadership in the GraphRAG revolution.

---

## 1. Academic Rigor + Industry Experience

### The Challenge

Most student teams bring either academic credentials **or** real-world experience, but rarely both. Most consulting firms bring industry expertise **or** research rigor, but rarely deep technical hands-on capability.

**Graphwise needs all three**: Academic publication standards + enterprise database expertise + ability to build and execute complex benchmarks.

### Our Solution: Triple Threat Team

**Academic Excellence**:
- **Average GPA**: 3.88/4.0 (all members above 3.85)
- **Publications**: IEEE Best Paper Award 2023 (Siddarth Bhave)
- **Teaching Experience**: Graduate Teaching Assistant (Mathew Jerry Meleth)
- **Research Training**: UW MSIM program emphasizes research methodology, statistical analysis, academic writing

**Industry Experience**:
- **14+ years combined** across AWS, Morgan Stanley, SAP, Rocket Mortgage, Mu Sigma, Adobe, o9 Solutions
- **Fortune 500 exposure**: 15+ clients (Rahil's consulting work)
- **Enterprise-scale databases**: AWS DynamoDB internship (Siddarth), multi-terabyte datasets (Mathew)
- **Global implementations**: 20+ supply chain platform deployments (Shreyas)

**Technical Execution Capability**:
- **Cloud platforms**: AWS (Lambda, S3, Glue, DynamoDB), Azure (Databricks, Data Factory)
- **Big data systems**: Hadoop, PySpark, Spark SQL, Kafka (75M records/minute throughput)
- **Performance optimization**: Every member has 30-90% improvement track record
- **Database breadth**: SQL (MySQL, PostgreSQL), NoSQL (MongoDB, Cassandra, DynamoDB), big data, distributed systems

### Value to Graphwise

**1. Methodology Credibility**:
- **Academic rigor** ensures peer-review quality (suitable for ISWC, VLDB submission)
- **Industry relevance** ensures enterprise decision-makers trust findings (not academic toy problems)

**2. Technical Depth**:
- **Siddarth's AWS DynamoDB internship** means he understands database internals, not just APIs
- **Mathew's 35% data ingestion optimization** proves he can measure performance accurately
- **Shreyas's 20+ global implementations** demonstrates he understands real-world deployment complexity

**3. Publication Potential**:
- **Siddarth's IEEE Best Paper Award** proves team can meet scholarly standards
- **Conference acceptance = media coverage** = thought leadership positioning for Graphwise
- **Academic citations = industry credibility** for years beyond initial publication

**Competitive Advantage**: No consulting firm can match our academic credentials. No pure student team can match our industry depth.

---

## 2. Vendor-Neutral Objectivity

### The Challenge

**Vendor-sponsored research** lacks credibility. Enterprise buyers discount whitepaper claims because:
- Benchmarks cherry-pick favorable queries
- Hardware configurations favor sponsor platform
- Competitors not tested at optimal configurations
- Results lack third-party validation

**Example**: "Fastest graph database" claims are ubiquitous but unverifiable. Every vendor claims #1 performance on different dimensions.

### Our Solution: University Independence

**No Conflicts of Interest**:
- **Zero relationships** with Graphwise, Stardog, Neo4j, AWS Neptune, Franz Inc., Fluree, TigerGraph
- **Academic institution affiliation** (University of Washington) ensures research integrity
- **No financial incentive** to favor any vendor (capstone project, not paid consultancy)
- **Faculty oversight** provides additional objectivity check

**Methodological Fairness**:
- **Independent review**: 2 external graph database experts validate query equivalence
- **Platform-optimal configurations**: Each vendor's recommended setup, not handicapped
- **Transparent limitations**: We document what we couldn't test and why
- **Open-source benchmark suite**: Community can replicate and validate findings

**Statistical Rigor**:
- **Pre-registered methodology**: Benchmark specification documented before execution (prevents p-hacking)
- **Significance testing**: 95% confidence intervals, effect sizes, multiple comparison corrections
- **Outlier transparency**: Anomalies investigated and reported honestly

### Value to Graphwise

**1. Sales Credibility**:
- **"University of Washington research shows..."** carries more weight than "Graphwise claims..."
- **Independent third-party** validation is gold standard for enterprise procurement
- **Analyst relations**: Gartner/Forrester value peer-reviewed research over vendor whitepapers

**2. Competitive Differentiation**:
- **Honest assessment** of when Graphwise wins vs. when competitors excel
- **Use case fit matrix**: Helps sales qualify prospects (don't waste time on poor-fit scenarios)
- **Technical credibility**: Detailed methodology demonstrates sophistication beyond marketing fluff

**3. Long-Term Value**:
- **Academic citations** continue driving credibility for years
- **Benchmark replication** by others validates findings repeatedly
- **Community trust**: Open-source suite signals confidence in results

**Competitive Advantage**: Vendor-sponsored research = inherent skepticism. University research = presumption of objectivity.

---

## 3. Performance Benchmarking Expertise

### The Challenge

Benchmarking is **not trivial**:
- Statistical significance requires large sample sizes and proper experimental design
- Platform configuration nuances dramatically impact results
- Query equivalence across SPARQL/Cypher/Gremlin is subtle
- Interpreting performance data requires deep systems understanding

**Example**: Data.world's LLM accuracy study (238% improvement with knowledge graphs) succeeded because:
1. Realistic enterprise scenario (insurance database)
2. Rigorous methodology (zero-shot GPT-4, validated ground truth)
3. Statistical validation (multiple runs, expert review)
4. Transparent limitations (single platform, limited query diversity)

### Our Solution: Track Record of Optimization

**Every Team Member** has quantified performance improvement experience:

**Rahil M. Harihar**:
- **Cycle time reduction**: 48 hours → minutes (90%+ improvement) for ML model deployment
- **100+ SAP integrations**: Deep understanding of enterprise data architecture performance patterns
- **Method**: Containerization, CI/CD optimization, architectural redesign

**Siddarth Bhave**:
- **$1M cost savings**: Network monitoring system optimization at Morgan Stanley
- **Query optimization**: 5 minutes → 45 seconds (90% improvement)
- **50% metric retention increase**: Storage and cache optimization
- **Method**: Database indexing, query plan analysis, distributed system tuning

**Mathew Jerry Meleth**:
- **35% data ingestion improvement**: Multi-terabyte AWS serverless pipelines
- **93% processing time reduction**: 1 month → 2 days (Azure Databricks automation)
- **30% retrieval speed improvement**: MapReduce/Hive optimization for YouTube analytics
- **Method**: Pipeline architecture, parallel processing, query optimization

**Shreyas B Subramanya**:
- **70% batch-run time reduction**: Supply chain data processing at o9 Solutions
- **35% issue resolution improvement**: Operational process optimization
- **Method**: Systematic workflow analysis, automation, performance monitoring

**Combined Expertise**: 30-90% improvements across **7 different companies**, spanning cloud platforms, big data systems, databases, and enterprise software.

### Value to Graphwise

**1. Measurement Accuracy**:
- **Experience matters**: We know how to eliminate cache bias, warm-up properly, handle outliers
- **Statistical rigor**: Power analysis for sample sizes, significance testing, effect size calculation
- **Replicability**: Documented methodology allows others to validate findings

**2. Insightful Analysis**:
- **Not just numbers**: We explain *why* Graphwise reasoning is faster (or slower) than competitors
- **Architectural understanding**: Siddarth's AWS DynamoDB internship = database internals expertise
- **Trade-off identification**: When is RDF complexity justified vs. when property graphs win?

**3. Actionable Recommendations**:
- **Performance bottleneck identification**: Where should Graphwise invest in optimization?
- **Feature gaps**: What capabilities do competitors have that Graphwise lacks (or vice versa)?
- **Pricing implications**: TCO analysis informs enterprise license negotiations

**Competitive Advantage**: Academic teams often lack performance optimization experience. Consulting firms often lack hands-on database expertise. We have both.

---

## 4. Proven Collaboration & Execution

### The Challenge

**Complex research projects fail** due to:
- Miscommunication between team members
- Misaligned work styles and expectations
- Inability to resolve conflicts
- Lack of accountability and follow-through

**Graphwise's risk**: 20-week timeline with 4 students = coordination overhead. If team chemistry is poor, deliverables suffer.

### Our Solution: Battle-Tested Partnership

**Rahil & Siddarth: Co-Founders of AaMaRa Technologies**:
- **Achievement**: ₹3M profit (bootstrapped, no external funding)
- **Duration**: Multi-year partnership building successful technology services company
- **Evidence**: Fortune 500 clients, profitable exit demonstrates:
  - **Trust and communication**: Co-founders must align on strategy, execution, conflict resolution
  - **Complementary skills**: Rahil (product/business), Siddarth (engineering) division of labor
  - **Delivery under pressure**: Startups = resource constraints, tight deadlines, high stakes

**All 4 Members: Current UW MSIM Classmates**:
- **Established relationships**: Working together on coursework, projects since August 2024
- **Shared work ethic**: All 3.85+ GPA demonstrates commitment to excellence
- **Complementary specializations**:
  - Rahil: AI + Product/Project Management
  - Siddarth: Data systems and distributed computing
  - Mathew: Cloud and big data engineering
  - Shreyas: Analytics and operations

### Value to Graphwise

**1. Reduced Coordination Risk**:
- **Proven partnership** (Rahil + Siddarth) anchors team
- **Clear role divisions**: No overlap or gaps in responsibilities
- **Established communication patterns**: Weekly syncs already routine from current coursework

**2. Execution Confidence**:
- **AaMaRa success proves**: This team can deliver complex technical projects on time
- **Academic excellence proves**: This team maintains quality under deadline pressure (3.88 avg GPA)
- **20-week timeline realistic**: Modular phases allow parallel workstreams, built-in buffers

**3. Conflict Resolution**:
- **Startup experience**: Rahil & Siddarth have resolved co-founder disagreements successfully
- **Faculty oversight**: UW advisors provide external escalation path if needed
- **Shared incentive**: Team's grade depends on Graphwise satisfaction (capstone project)

**Competitive Advantage**: Newly formed student teams = coordination risk. Co-founders with proven track record = execution confidence.

---

## 5. Cost-Effectiveness & ROI

### The Challenge

**Consulting firm benchmarking** costs $150K-$500K+:
- McKinsey/BCG rates: $250-$500/hr × 1,250 hours = $312K-$625K
- Boutique tech consultancies: $150-$250/hr × 1,250 hours = $187K-$312K
- Vendor-sponsored research: "Free" but lacks credibility

**Graphwise needs**: Academic rigor + technical depth + vendor neutrality at affordable cost.

### Our Solution: Academic Capstone Value

**Team Effort**: ~1,250 hours over 20 weeks
- Rahil: 300 hours
- Siddarth: 350 hours
- Mathew: 320 hours
- Shreyas: 280 hours

**Consulting Firm Equivalent Value**:
- At $250/hr (McKinsey rates): **$312,500**
- At $150/hr (boutique firm): **$187,500**

**Actual Cost to Graphwise**:
- **Direct cost**: $0 (academic capstone project)
- **Indirect costs**:
  - NDA execution and legal review: ~$2K
  - GraphDB Enterprise license access (if not already available): ~$10K-$20K for 6-month academic trial
  - Technical SME time (2-4 hrs/month × 5 months): ~$5K-$10K internal cost

**Total Investment**: **$17K-$32K** for **$187K-$312K** value = **5.8x to 18.4x ROI**

### Value to Graphwise

**1. Budget-Friendly**:
- **No consulting fees**: Capstone project aligned with team's graduation requirements
- **Platform access**: Marginal cost (academic licenses, trials)
- **Lower risk**: If deliverables underperform, minimal sunk cost

**2. Comparable Quality**:
- **Academic rigor**: Peer-review standards match or exceed consulting firm reports
- **Technical depth**: Team's 14+ years database experience rivals boutique firms
- **Thought leadership**: Publication potential exceeds typical consulting deliverables

**3. Long-Term Value**:
- **Academic citations**: Continue generating credibility for years
- **Open-source benchmark**: Community adoption extends impact
- **Talent pipeline**: Team graduates May 2026, potential hiring candidates

**Competitive Advantage**: Consulting firms = high cost, variable quality. Vendor research = low credibility. University research = high quality, low cost, high credibility.

---

## 6. Expected Impact & Outcomes

### For Graphwise (Business Value)

**Immediate (Week 20 Delivery)**:
1. **Academic-Quality Research Paper** (25-35 pages)
   - Peer-review ready for ISWC, VLDB, or Semantic Web Journal submission
   - Methodology enabling replication by other researchers
   - Comprehensive performance results with statistical validation

2. **Competitive Intelligence Report** (20-25 pages)
   - Platform-by-platform comparison across 6 vendors
   - Use case fit matrix: when Graphwise wins vs. when competitors excel
   - TCO analysis: 3-year cost modeling for enterprise decision-making

3. **Sales Enablement Package**:
   - Executive summary (8-10 pages) for prospect conversations
   - One-page scorecards per platform
   - Decision tree flowchart (when to choose Graphwise vs. Neo4j vs. AWS Neptune)

4. **Open-Source Benchmark Suite** (GitHub repository)
   - Data generators (publishing, life sciences, Customer 360 scenarios)
   - Query suites (SPARQL, Cypher, Gremlin equivalents)
   - Evaluation scripts (Python, Jupyter notebooks)
   - Enables community validation and annual benchmark updates

**3-6 Months Post-Delivery**:
- **Sales adoption**: Research referenced in 10+ enterprise conversations
- **Media coverage**: 2+ industry publications (Database Trends, Big Data Wire, KMWorld)
- **Conference submission**: Peer review and potential acceptance (ISWC, VLDB, WWW)
- **Analyst relations**: Cited in Gartner Magic Quadrant or Forrester Wave research

**12-18 Months Post-Delivery**:
- **Academic citations**: 20-50+ citations establish thought leadership
- **Benchmark replication**: 2+ independent research teams validate findings
- **Standards influence**: Contribution to LDBC or W3C Community Group discussions
- **Market positioning**: Measurable increase in Graphwise consideration set for GraphRAG use cases

### For the Semantic Technologies Community

**1. Standardization Contribution**:
- **First unified RDF vs. property graph benchmark**: Addresses decades-long paradigm divide
- **GraphRAG evaluation framework**: Standardized methodology for LLM + knowledge graph testing
- **Open methodology**: Enables annual benchmark updates tracking vendor progress

**2. Academic Research**:
- **Publication-quality work**: Fills critical gap in benchmarking literature
- **Replicable framework**: Other researchers can validate, extend, or challenge findings
- **Citation potential**: Similar to data.world study (arXiv:2311.07509), our research becomes reference for future work

**3. Industry Best Practices**:
- **TCO modeling**: First comprehensive cost analysis beyond just query performance
- **Use case fit matrix**: Helps enterprises choose right platform for their specific needs
- **Developer productivity insights**: Quantifies SPARQL vs. Cypher learning curve trade-offs

### For Our Team (Academic & Career Value)

**Academic Outcomes**:
- **Capstone project** fulfilling UW MSIM graduation requirement
- **Publication opportunity**: Co-authorship on peer-reviewed paper
- **Research portfolio**: Demonstrates ability to conduct rigorous research

**Career Outcomes**:
- **Graphwise hiring pipeline**: Potential full-time opportunities post-graduation (May 2026)
- **Industry network**: Connections with semantic technologies community
- **Differentiated resume**: Few students have academic publication + enterprise benchmarking experience

**Alignment**: Team's academic/career goals = Graphwise business goals = win-win partnership

---

## 7. Risk Mitigation & Contingencies

### Risk 1: Platform Access Limitations

**Scenario**: Unable to obtain academic licenses or free trials for all 6 platforms

**Mitigation**:
- **Enterprise Knowledge partnership**: Graphwise's consulting partner may provide access
- **Vendor outreach**: Academic research often qualifies for special licensing
- **Fallback**: Prioritize top 4 platforms (Graphwise, Stardog, AWS Neptune, Neo4j) if budget constraints

**Owner**: Shreyas (vendor coordination experience from 20+ implementations)

### Risk 2: Technical Complexity of Semantic Technologies

**Scenario**: Team lacks deep SPARQL, OWL, RDF expertise initially

**Mitigation**:
- **Siddarth's CS background**: 9.22/10 GPA, strong theoretical foundation for learning semantic web standards
- **UW faculty advisors**: Database systems experts available for consultation
- **2-3 week ramp-up**: Built into Week 1-2 timeline for self-study

**Contingency**: If learning curve steeper than expected, reduce scope to 4 platforms instead of 6

**Owner**: Siddarth (lead technical learning)

### Risk 3: Statistical Significance

**Scenario**: Performance differences too small to be statistically significant

**Mitigation**:
- **Large sample sizes**: 100 runs per query provides statistical power
- **Effect size calculation**: Even small differences with large effect sizes are meaningful
- **Qualitative analysis**: Feature comparison and use case fit still valuable if performance parity

**Contingency**: Extend testing by 1-2 weeks if initial results inconclusive

**Owner**: Mathew (statistical analysis expertise)

### Risk 4: Timeline Constraints

**Scenario**: Unexpected delays (technical issues, data quality problems, platform bugs)

**Mitigation**:
- **Modular timeline**: Parallel workstreams reduce critical path dependencies
- **Weekly check-ins**: Early identification of blockers
- **Built-in buffers**: 2-week slack across 20-week plan

**Contingency**: Can extend to Week 22 if critical issues arise (still within academic term)

**Owner**: Rahil (project management, risk tracking)

---

## 8. Why Now: Market Timing

### The GraphRAG Inflection Point (2024-2025)

**Microsoft GraphRAG** (July 2024):
- 20,000+ GitHub stars in 4 months
- Demonstrates 87% accuracy vs. 23% for traditional RAG
- Proves market demand for knowledge graph + LLM integration

**Data.world Study** (November 2024):
- Independent research: 238% LLM accuracy improvement (16% → 54%)
- Widely cited, media coverage, influenced enterprise procurement
- **Gap**: Only one platform tested, methodology not replicated across vendors

**SAP Knowledge Graph** (October 2024):
- Mainstream ERP adopts RDF/OWL, bringing semantic web to Fortune 500 masses
- Signals enterprise readiness for semantic technologies
- **Opportunity**: Standardized benchmarks now critical as market expands

**Graphwise GraphDB 11** (July 2025):
- LLM integration, Model Context Protocol, "Talk to Your Graph"
- **Need**: Third-party validation of GraphRAG superiority vs. Neo4j LLM Knowledge Graph Builder

### Gartner Hype Cycle: Slope of Enlightenment

**Knowledge graphs** reached "Slope of Enlightenment" in 2024 Hype Cycle for AI:
- **Meaning**: Moving from hype to proven, production deployments
- **Challenge**: Only 20% of organizations successfully deployed enterprise knowledge graphs
- **Opportunity**: Enterprises need credible evaluation frameworks to de-risk investments

**Graphwise FY2026 Planning**:
- **Timing**: 20-week study completes Q2 2026, aligned with strategic planning cycle
- **Impact**: Benchmark data informs product roadmap, pricing, go-to-market strategy

**Competitive Window**: First comprehensive RDF vs. property graph benchmark = thought leadership positioning before competitors publish

---

## 9. Success Metrics

### Research Quality

- **Conference acceptance**: ISWC, VLDB, WWW, or Semantic Web Journal publication
- **Peer review scores**: 4.0+ / 5.0 average
- **Methodology replication**: 2+ independent teams validate findings within 12 months
- **Benchmark adoption**: GitHub repo achieves 100+ stars, 20+ forks within 6 months

### Business Impact (Graphwise)

- **Sales enablement**: Referenced in 10+ enterprise conversations within 3 months
- **Media coverage**: 2+ industry publications (Database Trends, Big Data Wire, KMWorld)
- **Analyst relations**: Cited in Gartner Magic Quadrant or Forrester research
- **Lead generation**: 5+ qualified enterprise leads from thought leadership positioning

### Community Contribution

- **Academic citations**: 50+ within 18 months
- **Conference presentations**: 2+ workshops or talks (ISWC, VLDB satellite events)
- **Vendor engagement**: 3+ semantic technology vendors request collaboration
- **Standards influence**: LDBC or W3C Community Group discussions reference findings

### Quantifiable ROI

**Investment** (Graphwise):
- Direct cost: $0 (academic project)
- Indirect cost: $17K-$32K (platform access, SME time)

**Returns**:
- **Consulting value**: $187K-$312K (if purchased from McKinsey/boutique firm)
- **Sales acceleration**: If benchmark shortens 2 deals by 3 months each → $500K-$1M+ revenue pull-forward
- **Media value**: Industry coverage equivalent to $50K-$100K PR spend

**ROI Calculation**: 5.8x to 18.4x direct ROI, with potential 15x+ if sales impact realized

---

## 10. Call to Action

### What Graphwise Gains

**Academic Credibility**:
- Peer-review-ready research establishing thought leadership in semantic technologies
- University of Washington affiliation = vendor-neutral objectivity
- Publication potential = long-term citation and industry influence

**Competitive Intelligence**:
- Quantified performance data across 6 platforms informing FY2026 strategy
- Use case fit matrix guiding sales qualification and product positioning
- TCO analysis supporting enterprise pricing and ROI conversations

**Sales Enablement**:
- Independent validation replacing marketing claims with empirical evidence
- Executive summaries, one-pagers, decision trees ready for prospect conversations
- Media coverage and analyst citations accelerating enterprise consideration

### What We Deliver (Week 20)

1. **Academic Paper** (25-35 pages, peer-review ready)
2. **Executive Summary** (8-10 pages, sales-focused)
3. **Competitive Intelligence Report** (20-25 pages, platform-by-platform analysis)
4. **Open-Source Benchmark Suite** (GitHub: data, queries, scripts, methodology)
5. **Interactive Dashboards** (Power BI/Tableau for internal exploration)
6. **Final Presentation** (60-min executive briefing + 30-min Q&A)

### What Graphwise Invests

**Time Commitment**:
- Kickoff meeting (Week 1): 2 hours
- Methodology review (Week 6): 2 hours
- Interim results (Week 12): 2 hours
- Final presentation (Week 20): 2 hours
- **Total**: 8 hours executive time over 20 weeks

**Platform Access**:
- GraphDB Enterprise license (6-month academic trial)
- Technical SME availability (2-4 hours/month for questions)

**Financial**:
- NDA legal review: ~$2K
- Platform access (if new licenses needed): $10K-$20K
- **Total**: $12K-$22K

**ROI**: $187K-$312K consulting value for $12K-$22K investment = **8.5x to 14.3x ROI**

### Next Steps

**Week 1**:
1. NDA execution
2. Platform access provisioning (GraphDB Enterprise)
3. Kickoff meeting with Graphwise technical SMEs
4. Team begins literature review

**Week 6**:
- Methodology review and approval

**Week 12**:
- Interim results presentation

**Week 20**:
- Final delivery and executive briefing

---

## Conclusion: Why Choose This Team

**The Market Opportunity**: $48.4B semantic web market by 2030, driven by GraphRAG revolution

**The Positioning Challenge**: No standardized benchmarks validate Graphwise's technical superiority

**Our Unique Value**:
1. **Academic Rigor + Industry Experience**: 3.88 GPA + IEEE publication + 14 years enterprise database expertise
2. **Vendor-Neutral Objectivity**: University affiliation + no conflicts = credible third-party validation
3. **Performance Benchmarking Expertise**: Every member has 30-90% optimization track record
4. **Proven Collaboration**: Co-founded ₹3M profit startup demonstrates execution capability
5. **Cost-Effectiveness**: $187K-$312K consulting value for $12K-$22K investment

**The Bottom Line**: You get McKinsey/BCG-quality research at academic pricing, delivered by a battle-tested team with the perfect blend of scholarly rigor and real-world database expertise.

**Graphwise will establish thought leadership in semantic technologies, accelerate FY2026 sales cycles, and position for market leadership in the GraphRAG era.**

**The time is now. The team is ready. Let's make Graphwise the definitive authority in semantic layer benchmarking.**

---

**Contact Information**:
University of Washington MSIM Team - **The Big 4**
Knowledge Graph API: https://kg-student-backend.ambitiouswave-220155c4.eastus2.azurecontainerapps.io
Team Portal: https://purple-ocean-0a69d8a0f.3.azurestaticapps.net
